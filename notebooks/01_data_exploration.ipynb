{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNC Data Exploration\n",
    "\n",
    "This notebook explores the CNC manufacturing data from the CIMCO MDC Software database to understand patterns, data quality, and relationships that will inform our machine learning models.\n",
    "\n",
    "## Objectives\n",
    "1. Connect to the MySQL database and load data\n",
    "2. Perform initial data quality assessment\n",
    "3. Explore data distributions and patterns\n",
    "4. Identify key relationships for modeling\n",
    "5. Generate insights for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.database import DatabaseManager\n",
    "from data.preprocessing import DataPreprocessor\n",
    "from utils.helpers import validate_data_quality, setup_logging\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database connection\n",
    "print(\"Connecting to database...\")\n",
    "db = DatabaseManager()\n",
    "\n",
    "# Test connection\n",
    "connection_status = db.test_connection()\n",
    "print(f\"Database connection: {'Success' if connection_status else 'Failed'}\")\n",
    "\n",
    "if connection_status:\n",
    "    # Get table information\n",
    "    table_info = db.get_table_info()\n",
    "    print(f\"\\nTable: {table_info['table_name']}\")\n",
    "    print(f\"Columns: {table_info['column_count']}\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(\"\\nColumn Information:\")\n",
    "    for col, info in table_info['columns'].items():\n",
    "        print(f\"  {col}: {info['type']} {'(nullable)' if info['nullable'] else '(required)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (start with a sample for exploration)\n",
    "print(\"Loading data sample...\")\n",
    "df_sample = db.get_all_data(limit=5000)  # Start with 5k records\n",
    "\n",
    "print(f\"Loaded {len(df_sample)} records\")\n",
    "print(f\"Date range: {df_sample['StartTime'].min()} to {df_sample['StartTime'].max()}\")\n",
    "\n",
    "# Display basic info\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive data quality validation\n",
    "quality_report = validate_data_quality(df_sample)\n",
    "\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records: {quality_report['total_records']:,}\")\n",
    "print(f\"Duplicate records: {quality_report['duplicate_records']:,}\")\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "for col, missing in quality_report['missing_values'].items():\n",
    "    if missing > 0:\n",
    "        pct = (missing / quality_report['total_records']) * 100\n",
    "        print(f\"  {col}: {missing:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nNegative Duration Values:\")\n",
    "for col, negative in quality_report['negative_durations'].items():\n",
    "    pct = (negative / quality_report['total_records']) * 100\n",
    "    print(f\"  {col}: {negative:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nInvalid dates (1969): {quality_report['invalid_dates']:,}\")\n",
    "\n",
    "print(\"\\nOutliers:\")\n",
    "for col, outliers in quality_report['outliers'].items():\n",
    "    pct = (outliers / quality_report['total_records']) * 100\n",
    "    print(f\"  {col}: {outliers:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values visualization\n",
    "missing_data = df_sample.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_data.plot(kind='bar')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Missing Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the sample data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(\"NUMERICAL COLUMNS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "numerical_cols = df_sample.select_dtypes(include=[np.number]).columns\n",
    "df_sample[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns summary\n",
    "print(\"CATEGORICAL COLUMNS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "categorical_cols = ['machine', 'State', 'OperatorName', 'PartNumber']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_sample.columns:\n",
    "        unique_count = df_sample[col].nunique()\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "        \n",
    "        if unique_count < 20:  # Show all if less than 20\n",
    "            value_counts = df_sample[col].value_counts()\n",
    "            print(\"  Distribution:\")\n",
    "            for value, count in value_counts.head(10).items():\n",
    "                print(f\"    {value}: {count}\")\n",
    "        else:\n",
    "            print(\"  Top 5 most common:\")\n",
    "            value_counts = df_sample[col].value_counts()\n",
    "            for value, count in value_counts.head(5).items():\n",
    "                print(f\"    {value}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate efficiency if not present\n",
    "if 'efficiency' not in df_sample.columns:\n",
    "    df_sample['efficiency'] = np.where(\n",
    "        df_sample['JobDuration'] > 0,\n",
    "        df_sample['RunningTime'] / df_sample['JobDuration'],\n",
    "        0\n",
    "    )\n",
    "    # Cap efficiency at 100%\n",
    "    df_sample['efficiency'] = df_sample['efficiency'].clip(0, 1)\n",
    "\n",
    "# Calculate total downtime\n",
    "downtime_cols = ['SetupTime', 'WaitingSetupTime', 'NotFeedingTime', 'AdjustmentTime',\n",
    "                'DressingTime', 'ToolingTime', 'EngineeringTime', 'MaintenanceTime',\n",
    "                'BuyInTime', 'BreakShiftChangeTime', 'IdleTime']\n",
    "\n",
    "available_downtime_cols = [col for col in downtime_cols if col in df_sample.columns]\n",
    "df_sample['total_downtime'] = df_sample[available_downtime_cols].sum(axis=1)\n",
    "\n",
    "# Calculate parts per hour if possible\n",
    "if 'PartsProduced' in df_sample.columns:\n",
    "    df_sample['parts_per_hour'] = np.where(\n",
    "        df_sample['RunningTime'] > 0,\n",
    "        df_sample['PartsProduced'] * 3600 / df_sample['RunningTime'],\n",
    "        0\n",
    "    )\n",
    "    # Handle infinite values\n",
    "    df_sample['parts_per_hour'] = df_sample['parts_per_hour'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(\"Calculated derived metrics: efficiency, total_downtime, parts_per_hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Performance Metrics Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Efficiency histogram\n",
    "axes[0,0].hist(df_sample['efficiency'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_title('Efficiency Distribution')\n",
    "axes[0,0].set_xlabel('Efficiency')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].axvline(df_sample['efficiency'].mean(), color='red', linestyle='--', label=f'Mean: {df_sample[\"efficiency\"].mean():.3f}')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Job duration distribution (log scale)\n",
    "axes[0,1].hist(np.log1p(df_sample['JobDuration']), bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0,1].set_title('Job Duration Distribution (log scale)')\n",
    "axes[0,1].set_xlabel('Log(Job Duration + 1)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Total downtime distribution\n",
    "axes[1,0].hist(df_sample['total_downtime'] / 3600, bins=50, alpha=0.7, color='salmon', edgecolor='black')\n",
    "axes[1,0].set_title('Total Downtime Distribution (Hours)')\n",
    "axes[1,0].set_xlabel('Total Downtime (Hours)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Parts per hour distribution (if available)\n",
    "if 'parts_per_hour' in df_sample.columns:\n",
    "    # Filter out extreme outliers for visualization\n",
    "    parts_filtered = df_sample[df_sample['parts_per_hour'] <= df_sample['parts_per_hour'].quantile(0.95)]\n",
    "    axes[1,1].hist(parts_filtered['parts_per_hour'], bins=50, alpha=0.7, color='gold', edgecolor='black')\n",
    "    axes[1,1].set_title('Parts per Hour Distribution')\n",
    "    axes[1,1].set_xlabel('Parts per Hour')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Parts per Hour\\nNot Available', ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine performance comparison\n",
    "machine_stats = df_sample.groupby('machine').agg({\n",
    "    'efficiency': ['mean', 'std', 'count'],\n",
    "    'JobDuration': 'mean',\n",
    "    'total_downtime': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "machine_stats.columns = ['_'.join(col).strip() for col in machine_stats.columns]\n",
    "machine_stats = machine_stats.rename(columns={\n",
    "    'efficiency_mean': 'avg_efficiency',\n",
    "    'efficiency_std': 'efficiency_std',\n",
    "    'efficiency_count': 'job_count',\n",
    "    'JobDuration_mean': 'avg_job_duration',\n",
    "    'total_downtime_mean': 'avg_total_downtime'\n",
    "})\n",
    "\n",
    "# Sort by efficiency\n",
    "machine_stats = machine_stats.sort_values('avg_efficiency', ascending=True)\n",
    "\n",
    "print(\"MACHINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(machine_stats)\n",
    "\n",
    "# Visualize machine performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Average efficiency by machine\n",
    "machine_stats['avg_efficiency'].plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Average Efficiency by Machine')\n",
    "axes[0].set_xlabel('Efficiency')\n",
    "\n",
    "# Job count by machine\n",
    "machine_stats['job_count'].plot(kind='barh', ax=axes[1], color='orange')\n",
    "axes[1].set_title('Job Count by Machine')\n",
    "axes[1].set_xlabel('Number of Jobs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Operator Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operator analysis (exclude unknown operators)\n",
    "if 'OperatorName' in df_sample.columns:\n",
    "    operator_data = df_sample[df_sample['OperatorName'] != 'Unknown'].copy()\n",
    "    \n",
    "    if len(operator_data) > 0:\n",
    "        operator_stats = operator_data.groupby('OperatorName').agg({\n",
    "            'efficiency': ['mean', 'std', 'count'],\n",
    "            'machine': 'nunique',\n",
    "            'PartNumber': 'nunique',\n",
    "            'SetupTime': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        # Flatten column names\n",
    "        operator_stats.columns = ['_'.join(col).strip() for col in operator_stats.columns]\n",
    "        operator_stats = operator_stats.rename(columns={\n",
    "            'efficiency_mean': 'avg_efficiency',\n",
    "            'efficiency_std': 'efficiency_std',\n",
    "            'efficiency_count': 'job_count',\n",
    "            'machine_nunique': 'machines_operated',\n",
    "            'PartNumber_nunique': 'unique_parts',\n",
    "            'SetupTime_mean': 'avg_setup_time'\n",
    "        })\n",
    "        \n",
    "        # Filter operators with at least 5 jobs\n",
    "        experienced_operators = operator_stats[operator_stats['job_count'] >= 5].copy()\n",
    "        \n",
    "        if len(experienced_operators) > 0:\n",
    "            # Calculate versatility score\n",
    "            experienced_operators['versatility_score'] = experienced_operators['machines_operated'] * experienced_operators['unique_parts']\n",
    "            \n",
    "            print(\"TOP OPERATORS PERFORMANCE (min 5 jobs)\")\n",
    "            print(\"=\"*60)\n",
    "            top_operators = experienced_operators.nlargest(10, 'avg_efficiency')\n",
    "            print(top_operators[['avg_efficiency', 'job_count', 'machines_operated', 'versatility_score']])\n",
    "            \n",
    "            # Visualize operator performance\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "            # Top 10 operators by efficiency\n",
    "            top_operators['avg_efficiency'].plot(kind='bar', ax=axes[0,0], color='lightcoral')\n",
    "            axes[0,0].set_title('Top 10 Operators by Efficiency')\n",
    "            axes[0,0].set_ylabel('Average Efficiency')\n",
    "            axes[0,0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Efficiency vs consistency scatter\n",
    "            experienced_operators['consistency_score'] = 1 / (1 + experienced_operators['efficiency_std'])\n",
    "            axes[0,1].scatter(experienced_operators['avg_efficiency'], experienced_operators['consistency_score'], \n",
    "                            s=experienced_operators['job_count']*2, alpha=0.7, color='green')\n",
    "            axes[0,1].set_xlabel('Average Efficiency')\n",
    "            axes[0,1].set_ylabel('Consistency Score')\n",
    "            axes[0,1].set_title('Efficiency vs Consistency (size = job count)')\n",
    "            \n",
    "            # Versatility distribution\n",
    "            axes[1,0].hist(experienced_operators['versatility_score'], bins=20, alpha=0.7, color='purple')\n",
    "            axes[1,0].set_xlabel('Versatility Score')\n",
    "            axes[1,0].set_ylabel('Count')\n",
    "            axes[1,0].set_title('Operator Versatility Distribution')\n",
    "            \n",
    "            # Versatility vs efficiency\n",
    "            axes[1,1].scatter(experienced_operators['versatility_score'], experienced_operators['avg_efficiency'], \n",
    "                            alpha=0.7, color='orange')\n",
    "            axes[1,1].set_xlabel('Versatility Score')\n",
    "            axes[1,1].set_ylabel('Average Efficiency')\n",
    "            axes[1,1].set_title('Versatility vs Efficiency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No operators with sufficient job history (>=5 jobs) found.\")\n",
    "    else:\n",
    "        print(\"No operator data available (all operators are 'Unknown').\")\n",
    "else:\n",
    "    print(\"No OperatorName column found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Downtime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downtime category analysis\n",
    "print(\"DOWNTIME ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate average downtime by category\n",
    "downtime_averages = df_sample[available_downtime_cols].mean() / 3600  # Convert to hours\n",
    "downtime_averages = downtime_averages.sort_values(ascending=False)\n",
    "\n",
    "print(\"Average Downtime by Category (Hours):\")\n",
    "for category, avg_time in downtime_averages.items():\n",
    "    print(f\"  {category}: {avg_time:.2f}\")\n",
    "\n",
    "# Visualize downtime categories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Average downtime by category\n",
    "downtime_averages.plot(kind='barh', ax=axes[0], color='salmon')\n",
    "axes[0].set_title('Average Downtime by Category')\n",
    "axes[0].set_xlabel('Hours')\n",
    "\n",
    "# Downtime distribution (total)\n",
    "downtime_total_hours = df_sample['total_downtime'] / 3600\n",
    "axes[1].hist(downtime_total_hours[downtime_total_hours <= downtime_total_hours.quantile(0.95)], \n",
    "            bins=50, alpha=0.7, color='lightblue')\n",
    "axes[1].set_title('Total Downtime Distribution')\n",
    "axes[1].set_xlabel('Total Downtime (Hours)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downtime correlation with efficiency\n",
    "if len(available_downtime_cols) > 0:\n",
    "    # Calculate correlation between downtime categories and efficiency\n",
    "    downtime_efficiency_corr = df_sample[available_downtime_cols + ['efficiency']].corr()['efficiency'].drop('efficiency')\n",
    "    downtime_efficiency_corr = downtime_efficiency_corr.sort_values()\n",
    "    \n",
    "    print(\"\\nCorrelation between Downtime Categories and Efficiency:\")\n",
    "    for category, corr in downtime_efficiency_corr.items():\n",
    "        print(f\"  {category}: {corr:.3f}\")\n",
    "    \n",
    "    # Visualize correlation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'green' for x in downtime_efficiency_corr.values]\n",
    "    downtime_efficiency_corr.plot(kind='barh', color=colors)\n",
    "    plt.title('Correlation: Downtime Categories vs Efficiency')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time-based Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based analysis\n",
    "if 'StartTime' in df_sample.columns:\n",
    "    # Convert to datetime and extract time features\n",
    "    df_sample['StartTime'] = pd.to_datetime(df_sample['StartTime'])\n",
    "    df_sample['hour'] = df_sample['StartTime'].dt.hour\n",
    "    df_sample['day_of_week'] = df_sample['StartTime'].dt.dayofweek\n",
    "    df_sample['date'] = df_sample['StartTime'].dt.date\n",
    "    \n",
    "    # Filter out invalid dates (1969)\n",
    "    valid_time_data = df_sample[df_sample['StartTime'].dt.year > 1970].copy()\n",
    "    \n",
    "    if len(valid_time_data) > 0:\n",
    "        print(\"TIME-BASED PATTERNS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Hourly patterns\n",
    "        hourly_stats = valid_time_data.groupby('hour').agg({\n",
    "            'efficiency': 'mean',\n",
    "            'JobDuration': 'count'\n",
    "        }).round(3)\n",
    "        \n",
    "        # Daily patterns\n",
    "        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        daily_stats = valid_time_data.groupby('day_of_week').agg({\n",
    "            'efficiency': 'mean',\n",
    "            'JobDuration': 'count'\n",
    "        }).round(3)\n",
    "        daily_stats.index = [day_names[i] for i in daily_stats.index]\n",
    "        \n",
    "        # Visualize time patterns\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Hourly efficiency pattern\n",
    "        hourly_stats['efficiency'].plot(kind='line', marker='o', ax=axes[0,0], color='blue')\n",
    "        axes[0,0].set_title('Efficiency by Hour of Day')\n",
    "        axes[0,0].set_xlabel('Hour')\n",
    "        axes[0,0].set_ylabel('Average Efficiency')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hourly job volume\n",
    "        hourly_stats['JobDuration'].plot(kind='bar', ax=axes[0,1], color='orange')\n",
    "        axes[0,1].set_title('Job Volume by Hour of Day')\n",
    "        axes[0,1].set_xlabel('Hour')\n",
    "        axes[0,1].set_ylabel('Job Count')\n",
    "        \n",
    "        # Daily efficiency pattern\n",
    "        daily_stats['efficiency'].plot(kind='bar', ax=axes[1,0], color='green')\n",
    "        axes[1,0].set_title('Efficiency by Day of Week')\n",
    "        axes[1,0].set_xlabel('Day')\n",
    "        axes[1,0].set_ylabel('Average Efficiency')\n",
    "        \n",
    "        # Daily job volume\n",
    "        daily_stats['JobDuration'].plot(kind='bar', ax=axes[1,1], color='purple')\n",
    "        axes[1,1].set_title('Job Volume by Day of Week')\n",
    "        axes[1,1].set_xlabel('Day')\n",
    "        axes[1,1].set_ylabel('Job Count')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nBest performing hours (by efficiency):\")\n",
    "        best_hours = hourly_stats['efficiency'].nlargest(3)\n",
    "        for hour, eff in best_hours.items():\n",
    "            print(f\"  {hour}:00 - {eff:.3f}\")\n",
    "            \n",
    "        print(\"\\nBest performing days (by efficiency):\")\n",
    "        best_days = daily_stats['efficiency'].nlargest(3)\n",
    "        for day, eff in best_days.items():\n",
    "            print(f\"  {day} - {eff:.3f}\")\n",
    "    else:\n",
    "        print(\"No valid time data found (all timestamps are invalid).\")\n",
    "else:\n",
    "    print(\"No StartTime column found for time-based analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical variables\n",
    "numerical_cols = df_sample.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_sample[numerical_cols].corr()\n",
    "\n",
    "# Focus on correlations with efficiency\n",
    "efficiency_corr = correlation_matrix['efficiency'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"VARIABLES MOST CORRELATED WITH EFFICIENCY\")\n",
    "print(\"=\"*50)\n",
    "for var, corr in efficiency_corr.head(10).items():\n",
    "    if var != 'efficiency':\n",
    "        sign = '+' if correlation_matrix.loc['efficiency', var] > 0 else '-'\n",
    "        print(f\"  {var}: {sign}{corr:.3f}\")\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Select key variables for heatmap\n",
    "key_vars = ['efficiency', 'JobDuration', 'RunningTime', 'total_downtime', 'SetupTime', 'PartsProduced']\n",
    "key_vars = [var for var in key_vars if var in correlation_matrix.columns]\n",
    "\n",
    "if len(key_vars) > 1:\n",
    "    key_corr = correlation_matrix.loc[key_vars, key_vars]\n",
    "    sns.heatmap(key_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5, fmt='.3f')\n",
    "    plt.title('Correlation Heatmap - Key Performance Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient numerical variables for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key insights\n",
    "print(\"KEY INSIGHTS FROM DATA EXPLORATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Data quality insights\n",
    "print(\"\\n1. DATA QUALITY:\")\n",
    "if quality_report['invalid_dates'] > 0:\n",
    "    print(f\"   - Found {quality_report['invalid_dates']} records with invalid timestamps (1969)\")\n",
    "    print(\"   - Recommendation: Filter out 1969 timestamps in preprocessing\")\n",
    "\n",
    "if quality_report['negative_durations']:\n",
    "    print(\"   - Found negative duration values in some columns\")\n",
    "    print(\"   - Recommendation: Set negative durations to 0 or implement data validation\")\n",
    "\n",
    "# Performance insights\n",
    "print(\"\\n2. PERFORMANCE METRICS:\")\n",
    "print(f\"   - Average efficiency: {df_sample['efficiency'].mean():.3f}\")\n",
    "print(f\"   - Efficiency std dev: {df_sample['efficiency'].std():.3f}\")\n",
    "print(f\"   - {(df_sample['efficiency'] > 0.8).sum()} jobs ({(df_sample['efficiency'] > 0.8).mean()*100:.1f}%) have >80% efficiency\")\n",
    "\n",
    "# Machine insights\n",
    "print(\"\\n3. MACHINE PERFORMANCE:\")\n",
    "if len(machine_stats) > 0:\n",
    "    best_machine = machine_stats.index[-1]  # Last in sorted order (highest efficiency)\n",
    "    worst_machine = machine_stats.index[0]  # First in sorted order (lowest efficiency)\n",
    "    print(f\"   - Best performing machine: {best_machine} ({machine_stats.loc[best_machine, 'avg_efficiency']:.3f} efficiency)\")\n",
    "    print(f\"   - Worst performing machine: {worst_machine} ({machine_stats.loc[worst_machine, 'avg_efficiency']:.3f} efficiency)\")\n",
    "    efficiency_range = machine_stats['avg_efficiency'].max() - machine_stats['avg_efficiency'].min()\n",
    "    print(f\"   - Efficiency range across machines: {efficiency_range:.3f}\")\n",
    "\n",
    "# Operator insights\n",
    "if 'experienced_operators' in locals() and len(experienced_operators) > 0:\n",
    "    print(\"\\n4. OPERATOR PERFORMANCE:\")\n",
    "    print(f\"   - {len(experienced_operators)} operators with ≥5 jobs\")\n",
    "    best_operator = experienced_operators['avg_efficiency'].idxmax()\n",
    "    print(f\"   - Best operator: {best_operator} ({experienced_operators.loc[best_operator, 'avg_efficiency']:.3f} efficiency)\")\n",
    "    print(f\"   - Average operator versatility: {experienced_operators['versatility_score'].mean():.1f}\")\n",
    "\n",
    "# Time insights\n",
    "if 'hourly_stats' in locals():\n",
    "    print(\"\\n5. TIME PATTERNS:\")\n",
    "    best_hour = hourly_stats['efficiency'].idxmax()\n",
    "    worst_hour = hourly_stats['efficiency'].idxmin()\n",
    "    print(f\"   - Best performing hour: {best_hour}:00 ({hourly_stats.loc[best_hour, 'efficiency']:.3f} efficiency)\")\n",
    "    print(f\"   - Worst performing hour: {worst_hour}:00 ({hourly_stats.loc[worst_hour, 'efficiency']:.3f} efficiency)\")\n",
    "\n",
    "# Downtime insights\n",
    "print(\"\\n6. DOWNTIME ANALYSIS:\")\n",
    "main_downtime = downtime_averages.index[0] if len(downtime_averages) > 0 else \"Unknown\"\n",
    "print(f\"   - Primary downtime category: {main_downtime} ({downtime_averages.iloc[0]:.2f} hours average)\")\n",
    "print(f\"   - Total average downtime per job: {df_sample['total_downtime'].mean()/3600:.2f} hours\")\n",
    "\n",
    "print(\"\\n7. MODELING RECOMMENDATIONS:\")\n",
    "print(\"   - Use machine and operator as key categorical features\")\n",
    "print(\"   - Include time-based features (hour, day of week)\")\n",
    "print(\"   - Consider separate models for different downtime categories\")\n",
    "print(\"   - Implement data quality filters in preprocessing\")\n",
    "print(\"   - Focus on efficiency prediction as primary target\")\n",
    "print(\"   - Consider operator-machine interaction features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This exploration has provided valuable insights into the CNC manufacturing data:\n",
    "\n",
    "### Data Quality\n",
    "- Identified data quality issues that need preprocessing\n",
    "- Found patterns in missing values and outliers\n",
    "\n",
    "### Performance Patterns\n",
    "- Efficiency varies significantly across machines and operators\n",
    "- Clear time-based patterns exist in performance\n",
    "- Downtime categories have different impacts on overall efficiency\n",
    "\n",
    "### Modeling Opportunities\n",
    "- Strong candidates for predictive features identified\n",
    "- Multiple modeling targets possible (efficiency, downtime, duration)\n",
    "- Operator-machine combinations show promise for recommendation systems\n",
    "\n",
    "### Next Steps\n",
    "1. Implement comprehensive data preprocessing pipeline\n",
    "2. Engineer additional features based on insights\n",
    "3. Develop machine learning models for prediction\n",
    "4. Build operator performance analytics\n",
    "5. Create recommendation system for optimal assignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

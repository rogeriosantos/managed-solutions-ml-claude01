{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for CNC ML Project\n",
    "\n",
    "This notebook focuses on creating engineered features from the raw CNC data to improve machine learning model performance.\n",
    "\n",
    "## Objectives\n",
    "1. Apply data preprocessing pipeline\n",
    "2. Create time-based features\n",
    "3. Engineer performance metrics and ratios\n",
    "4. Build operator and machine interaction features\n",
    "5. Create rolling averages and trend features\n",
    "6. Prepare features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.database import DatabaseManager\n",
    "from data.preprocessing import DataPreprocessor\n",
    "from utils.helpers import setup_logging, create_time_features\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "print(\"Feature Engineering Notebook - CNC ML Project\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "db = DatabaseManager()\n",
    "df_raw = db.get_all_data(limit=10000)  # Load 10k records for feature engineering\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} records\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"Date range: {df_raw['StartTime'].min()} to {df_raw['StartTime'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and apply data preprocessor\n",
    "print(\"Applying data preprocessing...\")\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Validate and clean raw data\n",
    "df_clean = preprocessor.validate_raw_data(df_raw)\n",
    "print(f\"After cleaning: {len(df_clean)} records ({len(df_raw) - len(df_clean)} removed)\")\n",
    "\n",
    "# Create derived features\n",
    "df_features = preprocessor.create_derived_features(df_clean)\n",
    "print(f\"Features created. Dataset shape: {df_features.shape}\")\n",
    "\n",
    "# Display new features\n",
    "new_features = set(df_features.columns) - set(df_raw.columns)\n",
    "print(f\"\\nNew features created ({len(new_features)}):\")\n",
    "for feature in sorted(new_features):\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced time-based features\n",
    "print(\"Creating advanced time-based features...\")\n",
    "\n",
    "# Sort data by machine and time for sequential features\n",
    "df_features = df_features.sort_values(['machine', 'StartTime'])\n",
    "\n",
    "# Time since last job on same machine\n",
    "df_features['time_since_last_job'] = df_features.groupby('machine')['StartTime'].diff().dt.total_seconds()\n",
    "df_features['time_since_last_job'] = df_features['time_since_last_job'].fillna(0)\n",
    "\n",
    "# Job sequence number on machine\n",
    "df_features['machine_job_sequence'] = df_features.groupby('machine').cumcount() + 1\n",
    "\n",
    "# Time-based efficiency patterns\n",
    "if 'hour' in df_features.columns:\n",
    "    # Rush hour indicator (typically high production hours)\n",
    "    rush_hours = [8, 9, 10, 14, 15, 16]  # Common production peak hours\n",
    "    df_features['is_rush_hour'] = df_features['hour'].isin(rush_hours).astype(int)\n",
    "    \n",
    "    # Shift start/end indicators\n",
    "    df_features['shift_start'] = df_features['hour'].isin([6, 14, 22]).astype(int)  # Common shift start times\n",
    "    df_features['shift_end'] = df_features['hour'].isin([13, 21, 5]).astype(int)   # Common shift end times\n",
    "\n",
    "# Weekly patterns\n",
    "if 'day_of_week' in df_features.columns:\n",
    "    # Week position (beginning, middle, end)\n",
    "    df_features['week_position'] = df_features['day_of_week'].apply(\n",
    "        lambda x: 'start' if x <= 1 else ('middle' if x <= 3 else 'end')\n",
    "    )\n",
    "    \n",
    "    # Production day vs non-production day\n",
    "    df_features['production_day'] = (df_features['day_of_week'] < 5).astype(int)  # Monday-Friday\n",
    "\n",
    "print(f\"Time-based features created. New shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Ratio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance ratio features\n",
    "print(\"Creating performance ratio features...\")\n",
    "\n",
    "# Basic ratios (already created in preprocessing)\n",
    "print(f\"Efficiency range: {df_features['efficiency'].min():.3f} - {df_features['efficiency'].max():.3f}\")\n",
    "\n",
    "# Advanced performance ratios\n",
    "# Productive time ratio (RunningTime / (RunningTime + SetupTime))\n",
    "if 'SetupTime' in df_features.columns:\n",
    "    df_features['productive_time_ratio'] = (\n",
    "        df_features['RunningTime'] / \n",
    "        (df_features['RunningTime'] + df_features['SetupTime'] + 1)  # +1 to avoid division by zero\n",
    "    )\n",
    "\n",
    "# Downtime intensity (total_downtime / JobDuration)\n",
    "df_features['downtime_intensity'] = df_features['total_downtime'] / (df_features['JobDuration'] + 1)\n",
    "\n",
    "# Setup efficiency (1 / setup_time, normalized)\n",
    "if 'SetupTime' in df_features.columns:\n",
    "    df_features['setup_efficiency'] = 1 / (df_features['SetupTime'] / 1000 + 1)  # Normalized\n",
    "\n",
    "# Parts production rate relative to time\n",
    "if 'PartsProduced' in df_features.columns:\n",
    "    df_features['parts_per_minute'] = df_features['PartsProduced'] * 60 / (df_features['RunningTime'] + 1)\n",
    "    df_features['parts_per_minute'] = df_features['parts_per_minute'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(f\"Performance ratios created. Shape: {df_features.shape}\")\n",
    "\n",
    "# Display ratio statistics\n",
    "ratio_features = ['efficiency', 'productive_time_ratio', 'downtime_intensity', 'setup_efficiency']\n",
    "ratio_features = [f for f in ratio_features if f in df_features.columns]\n",
    "\n",
    "print(\"\\nPerformance Ratio Statistics:\")\n",
    "for feature in ratio_features:\n",
    "    mean_val = df_features[feature].mean()\n",
    "    std_val = df_features[feature].std()\n",
    "    print(f\"  {feature}: {mean_val:.3f} Â± {std_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine and Operator Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create machine-operator interaction features\n",
    "print(\"Creating machine-operator interaction features...\")\n",
    "\n",
    "# Historical performance by machine\n",
    "machine_historical = df_features.groupby('machine').agg({\n",
    "    'efficiency': ['mean', 'std'],\n",
    "    'JobDuration': 'mean',\n",
    "    'SetupTime': 'mean',\n",
    "    'total_downtime': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "machine_historical.columns = ['_'.join(col).strip() for col in machine_historical.columns]\n",
    "\n",
    "# Map machine historical features to dataset\n",
    "for col in machine_historical.columns:\n",
    "    feature_name = f'machine_hist_{col}'\n",
    "    df_features[feature_name] = df_features['machine'].map(machine_historical[col])\n",
    "\n",
    "# Historical performance by operator (excluding 'Unknown')\n",
    "if 'OperatorName' in df_features.columns:\n",
    "    operator_data = df_features[df_features['OperatorName'] != 'Unknown']\n",
    "    \n",
    "    if len(operator_data) > 0:\n",
    "        operator_historical = operator_data.groupby('OperatorName').agg({\n",
    "            'efficiency': ['mean', 'std'],\n",
    "            'SetupTime': 'mean',\n",
    "            'machine': 'nunique',\n",
    "            'PartNumber': 'nunique'\n",
    "        })\n",
    "        \n",
    "        operator_historical.columns = ['_'.join(col).strip() for col in operator_historical.columns]\n",
    "        \n",
    "        # Map operator historical features\n",
    "        for col in operator_historical.columns:\n",
    "            feature_name = f'operator_hist_{col}'\n",
    "            df_features[feature_name] = df_features['OperatorName'].map(operator_historical[col])\n",
    "            df_features[feature_name] = df_features[feature_name].fillna(df_features[feature_name].median())\n",
    "\n",
    "# Machine-operator combination experience\n",
    "if 'OperatorName' in df_features.columns:\n",
    "    df_features['machine_operator_jobs'] = df_features.groupby(['machine', 'OperatorName']).cumcount() + 1\n",
    "    \n",
    "    # Experience category\n",
    "    df_features['experience_category'] = pd.cut(\n",
    "        df_features['machine_operator_jobs'],\n",
    "        bins=[0, 1, 5, 15, float('inf')],\n",
    "        labels=['novice', 'beginner', 'experienced', 'expert']\n",
    "    )\n",
    "\n",
    "print(f\"Interaction features created. Shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rolling Window Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling window features for trend analysis\n",
    "print(\"Creating rolling window features...\")\n",
    "\n",
    "# Sort data for proper rolling calculations\n",
    "df_features = df_features.sort_values(['machine', 'OperatorName', 'StartTime'])\n",
    "\n",
    "# Rolling features for machine performance\n",
    "for window in [3, 5, 10]:\n",
    "    # Machine efficiency trends\n",
    "    df_features[f'machine_efficiency_roll_{window}'] = (\n",
    "        df_features.groupby('machine')['efficiency']\n",
    "        .rolling(window=window, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(0, drop=True)\n",
    "    )\n",
    "    \n",
    "    # Machine downtime trends\n",
    "    df_features[f'machine_downtime_roll_{window}'] = (\n",
    "        df_features.groupby('machine')['total_downtime']\n",
    "        .rolling(window=window, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(0, drop=True)\n",
    "    )\n",
    "\n",
    "# Rolling features for operator performance (if applicable)\n",
    "if 'OperatorName' in df_features.columns:\n",
    "    for window in [3, 5]:\n",
    "        # Operator efficiency trends\n",
    "        df_features[f'operator_efficiency_roll_{window}'] = (\n",
    "            df_features.groupby(['OperatorName'])['efficiency']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .mean()\n",
    "            .reset_index([0], drop=True)\n",
    "        )\n",
    "        \n",
    "        # Operator setup time trends\n",
    "        if 'SetupTime' in df_features.columns:\n",
    "            df_features[f'operator_setup_roll_{window}'] = (\n",
    "                df_features.groupby(['OperatorName'])['SetupTime']\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index([0], drop=True)\n",
    "            )\n",
    "\n",
    "# Trend features (difference from rolling mean)\n",
    "df_features['efficiency_vs_machine_trend'] = (\n",
    "    df_features['efficiency'] - df_features['machine_efficiency_roll_5']\n",
    ")\n",
    "\n",
    "if 'operator_efficiency_roll_3' in df_features.columns:\n",
    "    df_features['efficiency_vs_operator_trend'] = (\n",
    "        df_features['efficiency'] - df_features['operator_efficiency_roll_3']\n",
    "    )\n",
    "\n",
    "print(f\"Rolling features created. Shape: {df_features.shape}\")\n",
    "\n",
    "# Show trend feature statistics\n",
    "trend_features = [col for col in df_features.columns if 'trend' in col or 'roll' in col]\n",
    "print(f\"\\nCreated {len(trend_features)} trend/rolling features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Part Complexity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create part complexity and specialization features\n",
    "print(\"Creating part complexity features...\")\n",
    "\n",
    "if 'PartNumber' in df_features.columns:\n",
    "    # Filter out unknown parts\n",
    "    part_data = df_features[df_features['PartNumber'] != 'Unknown']\n",
    "    \n",
    "    if len(part_data) > 0:\n",
    "        # Part complexity based on average setup time\n",
    "        part_complexity = part_data.groupby('PartNumber').agg({\n",
    "            'SetupTime': 'mean',\n",
    "            'JobDuration': 'mean',\n",
    "            'efficiency': 'mean',\n",
    "            'machine': 'nunique',\n",
    "            'OperatorName': 'nunique'\n",
    "        })\n",
    "        \n",
    "        part_complexity.columns = ['avg_setup_time', 'avg_job_duration', 'avg_efficiency', 'machines_used', 'operators_used']\n",
    "        \n",
    "        # Calculate complexity scores\n",
    "        part_complexity['complexity_score'] = (\n",
    "            part_complexity['avg_setup_time'] / part_complexity['avg_setup_time'].max()\n",
    "        )\n",
    "        \n",
    "        part_complexity['versatility_score'] = (\n",
    "            part_complexity['machines_used'] * part_complexity['operators_used']\n",
    "        )\n",
    "        \n",
    "        # Map part features to main dataset\n",
    "        for col in part_complexity.columns:\n",
    "            feature_name = f'part_{col}'\n",
    "            df_features[feature_name] = df_features['PartNumber'].map(part_complexity[col])\n",
    "            # Fill missing values for 'Unknown' parts with median\n",
    "            df_features[feature_name] = df_features[feature_name].fillna(df_features[feature_name].median())\n",
    "        \n",
    "        # Part frequency (how often it's produced)\n",
    "        part_frequency = df_features['PartNumber'].value_counts()\n",
    "        df_features['part_frequency'] = df_features['PartNumber'].map(part_frequency)\n",
    "        df_features['part_frequency'] = df_features['part_frequency'].fillna(1)\n",
    "        \n",
    "        # Part popularity category\n",
    "        df_features['part_popularity'] = pd.cut(\n",
    "            df_features['part_frequency'],\n",
    "            bins=[0, 1, 5, 20, float('inf')],\n",
    "            labels=['rare', 'uncommon', 'common', 'frequent']\n",
    "        )\n",
    "        \n",
    "        print(f\"Part complexity features created for {len(part_complexity)} unique parts\")\n",
    "    else:\n",
    "        print(\"No valid part data found for complexity analysis\")\n",
    "else:\n",
    "    print(\"No PartNumber column found\")\n",
    "\n",
    "print(f\"Current dataset shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Anomaly and Quality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anomaly and quality indicator features\n",
    "print(\"Creating anomaly and quality features...\")\n",
    "\n",
    "# Extreme value indicators\n",
    "# Efficiency anomalies\n",
    "efficiency_q95 = df_features['efficiency'].quantile(0.95)\n",
    "efficiency_q05 = df_features['efficiency'].quantile(0.05)\n",
    "df_features['efficiency_extreme_high'] = (df_features['efficiency'] > efficiency_q95).astype(int)\n",
    "df_features['efficiency_extreme_low'] = (df_features['efficiency'] < efficiency_q05).astype(int)\n",
    "\n",
    "# Duration anomalies\n",
    "duration_q95 = df_features['JobDuration'].quantile(0.95)\n",
    "duration_q05 = df_features['JobDuration'].quantile(0.05)\n",
    "df_features['duration_extreme_long'] = (df_features['JobDuration'] > duration_q95 * 2).astype(int)\n",
    "df_features['duration_extreme_short'] = (df_features['JobDuration'] < duration_q05 / 2).astype(int)\n",
    "\n",
    "# Downtime anomalies\n",
    "downtime_q95 = df_features['total_downtime'].quantile(0.95)\n",
    "df_features['downtime_extreme_high'] = (df_features['total_downtime'] > downtime_q95 * 2).astype(int)\n",
    "\n",
    "# Quality indicators\n",
    "# Consistency indicators (based on rolling standard deviation)\n",
    "df_features['efficiency_consistency'] = (\n",
    "    df_features.groupby('machine')['efficiency']\n",
    "    .rolling(window=5, min_periods=2)\n",
    "    .std()\n",
    "    .reset_index(0, drop=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Stability score (inverse of consistency for better interpretation)\n",
    "df_features['stability_score'] = 1 / (1 + df_features['efficiency_consistency'])\n",
    "\n",
    "# Performance deviation from expected\n",
    "if 'machine_hist_efficiency_mean' in df_features.columns:\n",
    "    df_features['performance_deviation'] = (\n",
    "        abs(df_features['efficiency'] - df_features['machine_hist_efficiency_mean']) /\n",
    "        (df_features['machine_hist_efficiency_std'] + 0.01)  # Normalized by std dev\n",
    "    )\n",
    "\n",
    "# Rush job indicator (jobs with very short time since last job)\n",
    "df_features['rush_job'] = (df_features['time_since_last_job'] < 300).astype(int)  # Less than 5 minutes\n",
    "\n",
    "print(f\"Anomaly and quality features created. Shape: {df_features.shape}\")\n",
    "\n",
    "# Show anomaly statistics\n",
    "anomaly_features = [col for col in df_features.columns if 'extreme' in col or 'rush' in col]\n",
    "print(\"\\nAnomaly Feature Statistics:\")\n",
    "for feature in anomaly_features:\n",
    "    count = df_features[feature].sum()\n",
    "    pct = (count / len(df_features)) * 100\n",
    "    print(f\"  {feature}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection and Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance and correlations\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Select numerical features for analysis\n",
    "numerical_features = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove target-like features and identifiers for feature selection\n",
    "exclude_features = [\n",
    "    'efficiency', 'RunningTime', 'JobDuration', 'PartsProduced',\n",
    "    'StartTime', 'EndTime'  # These might be datetime but could appear as numbers\n",
    "]\n",
    "\n",
    "feature_candidates = [f for f in numerical_features if f not in exclude_features]\n",
    "\n",
    "print(f\"Analyzing {len(feature_candidates)} feature candidates for efficiency prediction\")\n",
    "\n",
    "# Calculate mutual information with efficiency (target)\n",
    "if len(feature_candidates) > 0:\n",
    "    # Prepare feature matrix (handle missing values)\n",
    "    X = df_features[feature_candidates].fillna(df_features[feature_candidates].median())\n",
    "    y = df_features['efficiency']\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "        \n",
    "        # Create feature importance dataframe\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_candidates,\n",
    "            'mutual_info_score': mi_scores\n",
    "        }).sort_values('mutual_info_score', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 15 Most Important Features (by Mutual Information):\")\n",
    "        print(feature_importance.head(15))\n",
    "        \n",
    "        # Visualize top features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance.head(20)\n",
    "        plt.barh(range(len(top_features)), top_features['mutual_info_score'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Mutual Information Score')\n",
    "        plt.title('Top 20 Feature Importance (Mutual Information with Efficiency)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating mutual information: {e}\")\n",
    "        feature_importance = None\n",
    "else:\n",
    "    print(\"No suitable features found for importance analysis\")\n",
    "    feature_importance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for feature selection\n",
    "print(\"\\nAnalyzing feature correlations...\")\n",
    "\n",
    "# Calculate correlations with efficiency\n",
    "efficiency_correlations = df_features[feature_candidates + ['efficiency']].corr()['efficiency'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 Features Correlated with Efficiency:\")\n",
    "for feature, corr in efficiency_correlations.head(16).items():  # 16 to exclude efficiency itself\n",
    "    if feature != 'efficiency':\n",
    "        sign = '+' if df_features[feature_candidates + ['efficiency']].corr()['efficiency'][feature] > 0 else '-'\n",
    "        print(f\"  {feature}: {sign}{corr:.3f}\")\n",
    "\n",
    "# Check for high correlation between features (multicollinearity)\n",
    "feature_corr_matrix = df_features[feature_candidates].corr().abs()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_corr_matrix.columns)):\n",
    "    for j in range(i+1, len(feature_corr_matrix.columns)):\n",
    "        corr_value = feature_corr_matrix.iloc[i, j]\n",
    "        if corr_value > 0.8:  # High correlation threshold\n",
    "            high_corr_pairs.append((\n",
    "                feature_corr_matrix.columns[i],\n",
    "                feature_corr_matrix.columns[j],\n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\nFound {len(high_corr_pairs)} highly correlated feature pairs (>0.8):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs[:10]:  # Show top 10\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo highly correlated feature pairs found (>0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare categorical features for modeling\n",
    "print(\"Encoding categorical features...\")\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = [\n",
    "    'machine', 'OperatorName', 'PartNumber', 'State',\n",
    "    'shift', 'week_position', 'experience_category', 'part_popularity'\n",
    "]\n",
    "\n",
    "# Filter to only include features that exist in the dataset\n",
    "categorical_features = [f for f in categorical_features if f in df_features.columns]\n",
    "\n",
    "print(f\"Encoding {len(categorical_features)} categorical features: {categorical_features}\")\n",
    "\n",
    "# Create encoders dictionary\n",
    "encoders = {}\n",
    "encoded_features = df_features.copy()\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in encoded_features.columns:\n",
    "        # Create label encoder\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Handle missing values by filling with 'Unknown'\n",
    "        encoded_features[feature] = encoded_features[feature].fillna('Unknown')\n",
    "        \n",
    "        # Encode the feature\n",
    "        encoded_features[f'{feature}_encoded'] = le.fit_transform(encoded_features[feature].astype(str))\n",
    "        \n",
    "        # Store encoder for later use\n",
    "        encoders[feature] = le\n",
    "        \n",
    "        # Print encoding info\n",
    "        unique_values = len(le.classes_)\n",
    "        print(f\"  {feature}: {unique_values} unique values encoded\")\n",
    "\n",
    "print(f\"\\nCategorical encoding complete. Dataset shape: {encoded_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final feature set for modeling\n",
    "print(\"Preparing final feature set...\")\n",
    "\n",
    "# Select modeling features (exclude raw categorical and identifier columns)\n",
    "exclude_for_modeling = [\n",
    "    'StartTime', 'EndTime', 'JobNumber', 'OpNumber', 'EmpID',\n",
    "    'machine', 'OperatorName', 'PartNumber', 'State',  # Raw categorical (we have encoded versions)\n",
    "    'shift', 'week_position', 'experience_category', 'part_popularity'  # Raw categorical\n",
    "]\n",
    "\n",
    "modeling_features = [col for col in encoded_features.columns if col not in exclude_for_modeling]\n",
    "\n",
    "# Create final modeling dataset\n",
    "df_modeling = encoded_features[modeling_features].copy()\n",
    "\n",
    "print(f\"Final modeling dataset shape: {df_modeling.shape}\")\n",
    "print(f\"Features for modeling: {len(modeling_features)}\")\n",
    "\n",
    "# Handle any remaining missing values\n",
    "missing_counts = df_modeling.isnull().sum()\n",
    "features_with_missing = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\nFeatures with missing values:\")\n",
    "    for feature, count in features_with_missing.items():\n",
    "        print(f\"  {feature}: {count} ({count/len(df_modeling)*100:.1f}%)\")\n",
    "    \n",
    "    # Fill missing values with median for numerical, mode for categorical\n",
    "    for feature in features_with_missing.index:\n",
    "        if df_modeling[feature].dtype in ['int64', 'float64']:\n",
    "            df_modeling[feature] = df_modeling[feature].fillna(df_modeling[feature].median())\n",
    "        else:\n",
    "            df_modeling[feature] = df_modeling[feature].fillna(df_modeling[feature].mode()[0])\n",
    "    \n",
    "    print(\"Missing values filled.\")\n",
    "else:\n",
    "    print(\"No missing values found in final dataset.\")\n",
    "\n",
    "# Data quality check\n",
    "print(f\"\\nFinal dataset validation:\")\n",
    "print(f\"  Shape: {df_modeling.shape}\")\n",
    "print(f\"  Missing values: {df_modeling.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(df_modeling.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"  Efficiency range: {df_modeling['efficiency'].min():.3f} - {df_modeling['efficiency'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories summary\n",
    "print(\"\\nFEATURE CATEGORIES SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "feature_categories = {\n",
    "    'Basic Performance': [f for f in modeling_features if f in ['efficiency', 'total_downtime', 'parts_per_hour']],\n",
    "    'Time Features': [f for f in modeling_features if any(t in f for t in ['hour', 'day', 'time', 'shift', 'week'])],\n",
    "    'Machine Features': [f for f in modeling_features if 'machine' in f],\n",
    "    'Operator Features': [f for f in modeling_features if 'operator' in f],\n",
    "    'Part Features': [f for f in modeling_features if 'part' in f],\n",
    "    'Rolling/Trend Features': [f for f in modeling_features if 'roll' in f or 'trend' in f],\n",
    "    'Ratio Features': [f for f in modeling_features if 'ratio' in f or 'intensity' in f],\n",
    "    'Anomaly Features': [f for f in modeling_features if 'extreme' in f or 'rush' in f],\n",
    "    'Quality Features': [f for f in modeling_features if any(q in f for q in ['consistency', 'stability', 'deviation'])],\n",
    "    'Encoded Categorical': [f for f in modeling_features if '_encoded' in f]\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        print(f\"\\n{category} ({len(features)} features):\")\n",
    "        for feature in features[:5]:  # Show first 5\n",
    "            print(f\"  - {feature}\")\n",
    "        if len(features) > 5:\n",
    "            print(f\"  ... and {len(features) - 5} more\")\n",
    "\n",
    "total_categorized = sum(len(features) for features in feature_categories.values())\n",
    "uncategorized = len(modeling_features) - total_categorized\n",
    "\n",
    "print(f\"\\nTotal categorized features: {total_categorized}\")\n",
    "print(f\"Uncategorized features: {uncategorized}\")\n",
    "print(f\"Total modeling features: {len(modeling_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed datasets\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "# Save feature-engineered dataset\n",
    "features_path = os.path.join(output_dir, 'features_engineered.csv')\n",
    "encoded_features.to_csv(features_path, index=False)\n",
    "print(f\"Feature-engineered dataset saved: {features_path}\")\n",
    "\n",
    "# Save modeling-ready dataset\n",
    "modeling_path = os.path.join(output_dir, 'modeling_ready.csv')\n",
    "df_modeling.to_csv(modeling_path, index=False)\n",
    "print(f\"Modeling-ready dataset saved: {modeling_path}\")\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'total_features': len(modeling_features),\n",
    "    'feature_categories': {cat: len(feats) for cat, feats in feature_categories.items() if feats},\n",
    "    'categorical_encoders': list(encoders.keys()),\n",
    "    'data_shape': df_modeling.shape,\n",
    "    'efficiency_stats': {\n",
    "        'mean': df_modeling['efficiency'].mean(),\n",
    "        'std': df_modeling['efficiency'].std(),\n",
    "        'min': df_modeling['efficiency'].min(),\n",
    "        'max': df_modeling['efficiency'].max()\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "info_path = os.path.join(output_dir, 'feature_info.json')\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2, default=str)\n",
    "print(f\"Feature information saved: {info_path}\")\n",
    "\n",
    "# Save encoders\n",
    "import pickle\n",
    "encoders_path = os.path.join(output_dir, 'categorical_encoders.pkl')\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "print(f\"Categorical encoders saved: {encoders_path}\")\n",
    "\n",
    "print(\"\\nAll processed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Feature Engineering Complete!\n",
    "\n",
    "This notebook has successfully created a comprehensive set of engineered features from the raw CNC data:\n",
    "\n",
    "#### Features Created:\n",
    "- **Time-based features**: Hour, day, shift patterns, rush hours, sequence numbers\n",
    "- **Performance ratios**: Efficiency metrics, productive time ratios, downtime intensity\n",
    "- **Historical features**: Machine and operator historical performance\n",
    "- **Rolling features**: Trend analysis with multiple window sizes\n",
    "- **Interaction features**: Machine-operator experience combinations\n",
    "- **Part complexity**: Setup complexity, popularity, versatility scores\n",
    "- **Anomaly indicators**: Extreme value detection, quality flags\n",
    "- **Categorical encoding**: Label-encoded categorical variables\n",
    "\n",
    "#### Key Outputs:\n",
    "1. **Feature-engineered dataset**: Complete dataset with all derived features\n",
    "2. **Modeling-ready dataset**: Clean dataset ready for ML training\n",
    "3. **Feature importance analysis**: Top predictive features identified\n",
    "4. **Categorical encoders**: Saved for consistent encoding in production\n",
    "\n",
    "#### Next Steps:\n",
    "1. **Model Development**: Use the prepared features to train ML models\n",
    "2. **Feature Selection**: Further refine feature set based on model performance\n",
    "3. **Hyperparameter Tuning**: Optimize models using the engineered features\n",
    "4. **Performance Validation**: Test models on hold-out data\n",
    "\n",
    "The feature engineering process has created a rich, informative dataset that should enable accurate predictions of efficiency, downtime, and optimal operator assignments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}